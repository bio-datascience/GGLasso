% Encoding: UTF-8

@Article{Friedman2007,
  author    = {J. Friedman and T. Hastie and R. Tibshirani},
  journal   = {Biostatistics},
  title     = {Sparse inverse covariance estimation with the graphical lasso},
  year      = {2007},
  month     = {dec},
  number    = {3},
  pages     = {432--441},
  volume    = {9},
  doi       = {10.1093/biostatistics/kxm045},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Yuan2007,
  author    = {M. Yuan and Y. Lin},
  journal   = {Biometrika},
  title     = {Model selection and estimation in the {G}aussian graphical model},
  year      = {2007},
  month     = {feb},
  number    = {1},
  pages     = {19--35},
  volume    = {94},
  doi       = {10.1093/biomet/asm018},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Danaher2013,
  author    = {Patrick Danaher and Pei Wang and Daniela M. Witten},
  journal   = {J. R. Stat. Soc. B},
  title     = {The joint graphical lasso for inverse covariance estimation across multiple classes},
  year      = {2013},
  month     = {aug},
  number    = {2},
  pages     = {373--397},
  volume    = {76},
  doi       = {10.1111/rssb.12033},
  publisher = {Wiley},
}

@Book{Lauritzen1996,
  author    = {Lauritzen, Steffen},
  publisher = {Clarendon Press Oxford University Press},
  title     = {Graphical models},
  year      = {1996},
  address   = {Oxford New York},
  isbn      = {9780198522195},
}

@Article{Kurtz2015,
  author    = {Zachary D. Kurtz and Christian L. Müller and Emily R. Miraldi and Dan R. Littman and Martin J. Blaser and Richard A. Bonneau},
  journal   = {{PLOS} Computational Biology},
  title     = {Sparse and {C}ompositionally {R}obust {I}nference of {M}icrobial {E}cological {N}etworks},
  year      = {2015},
  month     = {may},
  number    = {5},
  pages     = {e1004226},
  volume    = {11},
  doi       = {10.1371/journal.pcbi.1004226},
  editor    = {Christian von Mering},
  publisher = {Public Library of Science ({PLoS})},
}

@Article{Kurtz2019,
  author    = {Zachary D. Kurtz and Richard Bonneau and Christian L. Müller},
  title     = {Disentangling microbial associations from hidden environmental and technical factors via latent graphical models},
  year      = {2019},
  month     = {dec},
  doi       = {10.1101/2019.12.21.885889},
  publisher = {Cold Spring Harbor Laboratory},
}

@Article{Ma2013,
  author    = {Shiqian Ma and Lingzhou Xue and Hui Zou},
  journal   = {Neural Comput.},
  title     = {Alternating {D}irection {M}ethods for {L}atent {V}ariable {G}aussian {G}raphical {M}odel {S}election},
  year      = {2013},
  month     = {aug},
  number    = {8},
  pages     = {2172--2198},
  volume    = {25},
  doi       = {10.1162/neco_a_00379},
  publisher = {{MIT} Press - Journals},
}

@InProceedings{Tomasi2018,
  author    = {Federico Tomasi and Veronica Tozzo and Saverio Salzo and Alessandro Verri},
  booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery {\&} Data Mining},
  title     = {Latent {V}ariable {T}ime-varying {N}etwork {I}nference},
  year      = {2018},
  month     = {jul},
  publisher = {{ACM}},
  doi       = {10.1145/3219819.3220121},
}

@InProceedings{Hallac2017,
  author    = {David Hallac and Youngsuk Park and Stephen Boyd and Jure Leskovec},
  booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
  title     = {Network {I}nference via the {T}ime-{V}arying {G}raphical {L}asso},
  year      = {2017},
  month     = {aug},
  publisher = {{ACM}},
  doi       = {10.1145/3097983.3098037},
}

@Article{Chandrasekaran2012,
  author    = {Venkat Chandrasekaran and Pablo A. Parrilo and Alan S. Willsky},
  journal   = {Ann. Statist.},
  title     = {Latent variable graphical model selection via convex optimization},
  year      = {2012},
  month     = {aug},
  number    = {4},
  pages     = {1935--1967},
  volume    = {40},
  doi       = {10.1214/11-aos949},
  publisher = {Institute of Mathematical Statistics},
}

@Misc{Schaipp2020,
  author       = {Fabian Schaipp},
  howpublished = {\url{https://github.com/fabian-sp/GGLasso}},
  title        = {G{G}{L}asso: implementation of algorithms for Single and Multiple Graphical Lasso problems},
  year         = {2020},
}

@Misc{Friedman2019,
  author       = {Jerome Friedman and Trevor Hastie and Rob Tibshirani},
  howpublished = {\url{ https://CRAN.R-project.org/package=glasso}},
  title        = {Graphical Lasso: Estimation of Gaussian Graphical Models},
  year         = {2019},
}

%%%%%%%%%%%%%%%%%%%%%%
% URLs

@Misc{Danaher2018,
  author       = {Danaher, Patrick},
  howpublished = {\url{https://CRAN.R-project.org/package=JGL}},
  title        = {Performs the Joint Graphical Lasso for Sparse Inverse Covariance Estimation on Multiple Classes},
  year         = {2018},
}

@Article{Pedregosa2011,
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {J Mach Learn Res},
  title   = {Scikit-learn: {M}achine {L}earning in {P}ython},
  year    = {2011},
  pages   = {2825--2830},
  volume  = {12},
}

@Article{Zhang2019,
  author      = {Ning Zhang and Yangjing Zhang and Defeng Sun and Kim-Chuan Toh},
  title       = {An Efficient Linearly Convergent Regularized Proximal Point Algorithm for Fused Multiple Graphical Lasso Problems},
  year        = {2019},
  abstract    = {Nowadays, analysing data from different classes or over a temporal grid has attracted a great deal of interest. As a result, various multiple graphical models for learning a collection of graphical models simultaneously have been derived by introducing sparsity in graphs and similarity across multiple graphs. This paper focuses on the fused multiple graphical Lasso model which encourages not only shared pattern of sparsity, but also shared values of edges across different graphs. For solving this model, we develop an efficient regularized proximal point algorithm, where the subproblem in each iteration of the algorithm is solved by a superlinearly convergent semismooth Newton method. To implement the semismooth Newton method, we derive an explicit expression for the generalized Jacobian of the proximal mapping of the fused multiple graphical Lasso regularizer. Unlike those widely used first order methods, our approach has heavily exploited the underlying second order information through the semismooth Newton method. This can not only accelerate the convergence of the algorithm, but also improve its robustness. The efficiency and robustness of our proposed algorithm are demonstrated by comparing with some state-of-the-art methods on both synthetic and real data sets. Supplementary materials for this article are available online.},
  date        = {2019-02-19},
  eprint      = {1902.06952v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1902.06952v1:PDF},
  keywords    = {math.OC},
}

@Article{Simon2013,
  author    = {Noah Simon and Jerome Friedman and Trevor Hastie and Robert Tibshirani},
  journal   = {J. Comput. Graph. Statist.},
  title     = {A {S}parse-{G}roup {L}asso},
  year      = {2013},
  month     = {apr},
  number    = {2},
  pages     = {231--245},
  volume    = {22},
  doi       = {10.1080/10618600.2012.681250},
  publisher = {Informa {UK} Limited},
}

@InProceedings{Tozzo2020,
  author    = {Tozzo, Veronica and Barla, Annalisa},
  booktitle = {Complex Networks and Their Applications VIII},
  title     = {Multi-parameters {M}odel {S}election for {N}etwork {I}nference},
  year      = {2020},
  address   = {Cham},
  editor    = {Cherifi, Hocine and Gaito, Sabrina and Mendes, Jos{\'e} Fernendo and Moro, Esteban and Rocha, Luis Mateus},
  pages     = {566--577},
  publisher = {Springer International Publishing},
  abstract  = {Network inference is the reverse-engineering problem of inferring graphs from data. With the always increasing availability of data, methods based on probability assumptions that infer multiple intertwined networks have been proposed in literature. These methods, while being extremely flexible, have the major drawback of presenting a high number of hyper-parameters that need to be tuned. The tuning of hyper-parameters, in unsupervised settings, can be performed through criteria based on likelihood or stability. Likelihood-based scores can be easily generalised to the multi hyper-parameters setting, but their computation is feasible only under certain probability assumptions. Differently, stability-based methods are of general application and, on single hyper-parameter, they have been proved to outperform likelihood-based scores. In this work we present a multi-parameters extension to stability-based methods that can be easily applied on complex models. We extensively compared this extension with likelihood-based scores on synthetic Gaussian data. Experiments show that our extension provides a better estimate of models of increasing complexity providing a valuable alternative of existing likelihood-based model selection methods.},
  isbn      = {978-3-030-36687-2},
}

@Article{Boyd2011,
  author     = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
  journal    = {Found. Trends Mach. Learn.},
  title      = {Distributed {O}ptimization and {S}tatistical {L}earning via the {A}lternating {D}irection {M}ethod of {M}ultipliers},
  year       = {2011},
  issn       = {1935-8237},
  month      = jan,
  number     = {1},
  pages      = {1–122},
  volume     = {3},
  abstract   = {Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for ℓ1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.},
  address    = {Hanover, MA, USA},
  doi        = {10.1561/2200000016},
  issue_date = {January 2011},
  numpages   = {122},
  publisher  = {Now Publishers Inc.},
}

@Article{Zhang2020,
  author   = {Zhang, Yangjing and Zhang, Ning and Sun, Defeng and Toh, Kim-Chuan},
  journal  = {SIAM J. Optim.},
  title    = {A proximal point dual {N}ewton algorithm for solving group graphical {L}asso problems},
  year     = {2020},
  issn     = {1052-6234},
  number   = {3},
  pages    = {2197--2220},
  volume   = {30},
  doi      = {10.1137/19M1267830},
  fjournal = {SIAM Journal on Optimization},
  mrclass  = {90C22 (62J07 90C25 90C31)},
  mrnumber = {4134036},
}

@Article{Witten2011,
  author    = {Daniela M. Witten and Jerome H. Friedman and Noah Simon},
  journal   = {J. Comput. Graph. Statist.},
  title     = {New {I}nsights and {F}aster {C}omputations for the {G}raphical {L}asso},
  year      = {2011},
  number    = {4},
  pages     = {892-900},
  volume    = {20},
  doi       = {10.1198/jcgs.2011.11051a},
  eprint    = {https://doi.org/10.1198/jcgs.2011.11051a},
  publisher = {Taylor & Francis},
  url       = {https://doi.org/10.1198/jcgs.2011.11051a},
}

@InProceedings{Foygel2010,
  author    = {Foygel, Rina and Drton, Mathias},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Extended {B}ayesian {I}nformation {C}riteria for {G}aussian {G}raphical {M}odels},
  year      = {2010},
  editor    = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
  publisher = {Curran Associates, Inc.},
  volume    = {23},
  url       = {https://proceedings.neurips.cc/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf},
}

@Article{Hsieh2014,
  author     = {Hsieh, Cho-Jui and Sustik, M\'{a}ty\'{a}s A. and Dhillon, Inderjit S. and Ravikumar, Pradeep},
  journal    = {J. Mach. Learn. Res.},
  title      = {Q{UIC}: quadratic approximation for sparse inverse covariance estimation},
  year       = {2014},
  issn       = {1532-4435},
  pages      = {2911--2947},
  volume     = {15},
  fjournal   = {Journal of Machine Learning Research (JMLR)},
  mrclass    = {62H12 (62H99 62J07 90C90)},
  mrnumber   = {3277149},
  mrreviewer = {Xu-Qing Liu},
}

@Article{Meinshausen2006,
  author     = {Meinshausen, Nicolai and B\"{u}hlmann, Peter},
  journal    = {Ann. Statist.},
  title      = {High-dimensional graphs and variable selection with the lasso},
  year       = {2006},
  issn       = {0090-5364},
  number     = {3},
  pages      = {1436--1462},
  volume     = {34},
  doi        = {10.1214/009053606000000281},
  fjournal   = {The Annals of Statistics},
  mrclass    = {62J07 (62F12 62H20)},
  mrnumber   = {2278363},
  mrreviewer = {Ji\v{r}\'{\i} And\v{e}l},
}

@Article{Ravikumar2011,
  author     = {Ravikumar, Pradeep and Wainwright, Martin J. and Raskutti, Garvesh and Yu, Bin},
  journal    = {Electron. J. Stat.},
  title      = {High-dimensional covariance estimation by minimizing {$\ell_1$}-penalized log-determinant divergence},
  year       = {2011},
  pages      = {935--980},
  volume     = {5},
  doi        = {10.1214/11-EJS631},
  fjournal   = {Electronic Journal of Statistics},
  mrclass    = {62H12 (05C90 62F12 62F30 62J07)},
  mrnumber   = {2836766},
  mrreviewer = {Xu-Qing Liu},
}

@Article{Rockafellar1976,
  author     = {Rockafellar, R. T.},
  journal    = {Mathematics of Operations Research},
  title      = {Augmented {L}agrangians and applications of the proximal point algorithm in convex programming},
  year       = {1976},
  issn       = {0364-765X},
  number     = {2},
  pages      = {97--116},
  volume     = {1},
  doi        = {10.1287/moor.1.2.97},
  fjournal   = {Mathematics of Operations Research},
  groups     = {prox_point},
  mrclass    = {90C25},
  mrnumber   = {418919},
  mrreviewer = {B. Ross Barmish},
}

@Comment{jabref-meta: databaseType:bibtex;}
